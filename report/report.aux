\relax 
\bibstyle{ieeetr}
\babel@aux{english}{}
\citation{Carleo19}
\citation{Goodfellow16}
\citation{Carleo19}
\citation{Goodfellow16}
\citation{LeCun89}
\citation{Goodfellow16}
\citation{ILSVRC15}
\citation{Dierking03}
\citation{Sigaki20}
\citation{Minor20}
\citation{Sigaki20}
\citation{Demus99}
\citation{Dierking03}
\citation{Demus99}
\citation{Dierking03}
\citation{Demus99}
\citation{Dierking03}
\citation{Demus99}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background theory}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Liquid crystal phases}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Structure}{1}\protected@file@percent }
\citation{Dierking03}
\citation{Dierking03}
\citation{Dierking03}
\citation{Goodfellow16}
\citation{Murphy12}
\citation{Murphy12}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Polarised microscopy}{2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Liquid crystal textures from the dataset, from left to right: nematic phase compound 5CB, cholesteric phase compound D5, and smectic C phase compound M10.\relax }}{2}\protected@file@percent }
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Murphy12}
\citation{Goodfellow16}
\citation{Murphy12}
\citation{Goodfellow16}
\citation{Murphy12}
\citation{Goodfellow16}
\citation{Kaufman12}
\citation{Minsky69}
\citation{Rumelhart86}
\citation{Haykin98}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}General machine learning principles}{3}\protected@file@percent }
\citation{Rumelhart86}
\citation{Haykin98}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Feedforward neural networks}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Forward propagation}{4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Diagram of a feedforward fully-connected neural network with three input values, $D=3$, $W_1=W_2=4$, and one output unit.\relax }}{4}\protected@file@percent }
\citation{Haykin98}
\citation{Glorot11}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Hornik89}
\citation{Haykin98}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Haykin98}
\citation{Goodfellow16}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Training}{5}\protected@file@percent }
\citation{Goodfellow16}
\citation{Kline05}
\citation{Richard91}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Amari93}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Kingma14}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Bishop95}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Srivastava2014}
\citation{Ioffe15}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Regularisation}{7}\protected@file@percent }
\citation{Goodfellow16}
\citation{Shrestha19}
\citation{Bengio93}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Convolutional neural networks}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Convolutional layers}{8}\protected@file@percent }
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Shi16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Pooling layers}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Project work}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Model training set-up}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Universal training configurations and definitions}{9}\protected@file@percent }
\citation{Williams20a}
\citation{Williams20b}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Image data preparation}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Models I, 4-phase classifiers}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Dataset construction I}{10}\protected@file@percent }
\citation{Goodfellow16}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Dataset distribution for 4-phase classifier models.\relax }}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Model architectures and training configurations I}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Results I}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Plots of the mean accuracies against the number of convolutional layers for the 4-phase models over three training runs.\relax }}{12}\protected@file@percent }
\citation{Dierking03}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Test set confusion matrices for the models with the highest and lowest overall test accuracies. The value in each square represents the fraction of examples with the true phase label for which the model output was the predicted phase label. We, therefore, aim for values close to one along the downward diagonal, which corresponds to correct model outputs.\relax }}{13}\protected@file@percent }
\citation{ILSVRC15}
\citation{Szegedy2015}
\citation{Szegedy2015}
\citation{Szegedy2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Models II, smectic A and C binary classifiers}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Dataset construction II}{14}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Dataset distribution for smectic A and C classifier models.\relax }}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Model architectures and training configuration II}{14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Architectural diagram of an Inception block. The input is shared between 4 parallel convolution operations, with their outputs being concatenated along the channel axis in the final layer. This results in the output having a number of channels equal to the sum of the number of channels of each concatenated layer. Different kernel sizes in each parallel layer can extract differently sized features from the input, propagating more information through the output of the block. The $1 \times 1$ kernel layers preceding the $3 \times 3$ and $5 \times 5$ ones can be used to reduce the number of channels and, therefore, the computational cost, if necessary. The pooling layer is there to provide some translational invariance \cite  {Szegedy2015}.\relax }}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Results II}{15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Plots of the mean accuracies over three training runs against the number of convolutional layers for sequential models, and the number of Inception blocks for Inception models, for the smectic A and C models.\relax }}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Models III, general smectic classifiers}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Dataset construction III}{16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Test set confusion matrices for the smectic A and C models with the highest and lowest overall test accuracies.\relax }}{17}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Dataset distribution for general smectic classifier models.\relax }}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Model architectures and training configuration III}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Results III}{17}\protected@file@percent }
\citation{Aghdam17}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusions}{18}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Going forward}{18}\protected@file@percent }
\bibdata{report}
\bibcite{Carleo19}{1}
\bibcite{Goodfellow16}{2}
\bibcite{LeCun89}{3}
\bibcite{ILSVRC15}{4}
\bibcite{Dierking03}{5}
\bibcite{Sigaki20}{6}
\bibcite{Minor20}{7}
\bibcite{Demus99}{8}
\bibcite{Murphy12}{9}
\bibcite{Kaufman12}{10}
\bibcite{Minsky69}{11}
\bibcite{Rumelhart86}{12}
\bibcite{Haykin98}{13}
\bibcite{Glorot11}{14}
\bibcite{Hornik89}{15}
\bibcite{Kline05}{16}
\bibcite{Richard91}{17}
\bibcite{Amari93}{18}
\bibcite{Kingma14}{19}
\bibcite{Bishop95}{20}
\bibcite{Srivastava2014}{21}
\bibcite{Ioffe15}{22}
\bibcite{Shrestha19}{23}
\bibcite{Bengio93}{24}
\bibcite{Aghdam17}{25}
\bibcite{Shi16}{26}
\bibcite{Williams20a}{27}
\bibcite{Williams20b}{28}
\bibcite{Szegedy2015}{29}
\citation{Rumelhart86}
\citation{Goodfellow16}
\@writefile{toc}{\contentsline {section}{\numberline {A}Training algorithms}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Backpropagation}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Adam optimiser}{21}\protected@file@percent }
\citation{Kingma14}
\@writefile{toc}{\contentsline {section}{\numberline {B}Example architecture diagrams}{22}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Architectural diagram of the sequential CNN model with input size $256 \times 256$ and three convolutional layers. The output dimensions are displayed as width $\times $ height $\times $ channels for each convolutional and max pooling layer. Not to scale.\relax }}{23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Architectural diagram of the Inception CNN model with one block. The output dimensions are displayed as width $\times $ height $\times $ channels for each convolutional, max pooling and Inception block layer. Not to scale.\relax }}{24}\protected@file@percent }
