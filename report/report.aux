\relax 
\bibstyle{ieeetr}
\citation{Carleo19}
\citation{Goodfellow16}
\citation{Carleo19}
\citation{Goodfellow16}
\citation{LeCun89}
\citation{Goodfellow16}
\citation{ILSVRC15}
\citation{Dierking03}
\citation{Sigaki20}
\citation{Minor20}
\citation{Sigaki20}
\citation{Demus99}
\citation{Dierking03}
\citation{Demus99}
\citation{Dierking03}
\citation{Demus99}
\citation{Dierking03}
\citation{Demus99}
\citation{Dierking03}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Liquid crystal phases}{1}\protected@file@percent }
\citation{Dierking03}
\citation{Dierking03}
\citation{Goodfellow16}
\citation{Murphy12}
\citation{Murphy12}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Liquid crystal textures from the dataset, from left to right: nematic phase compound 5CB, cholesteric phase compound D5, and smectic C phase compound M10.\relax }}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}General machine learning principles}{2}\protected@file@percent }
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Murphy12}
\citation{Goodfellow16}
\citation{Murphy12}
\citation{Goodfellow16}
\citation{Murphy12}
\citation{Goodfellow16}
\citation{Kaufman12}
\citation{Minsky69}
\citation{Rumelhart86}
\citation{Haykin98}
\@writefile{toc}{\contentsline {section}{\numberline {4}Feedforward neural networks}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Forward propagation}{3}\protected@file@percent }
\citation{Rumelhart86}
\citation{Haykin98}
\citation{Haykin98}
\citation{Glorot11}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Diagram of a feedforward fully-connected neural network with three input values, $D=3$, $W_1=W_2=4$ and one output unit.\relax }}{4}\protected@file@percent }
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Hornik89}
\citation{Haykin98}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Haykin98}
\citation{Goodfellow16}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Training}{5}\protected@file@percent }
\citation{Goodfellow16}
\citation{Kline05}
\citation{Richard91}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Amari93}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Kingma14}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Bishop95}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Srivastava2014}
\citation{Goodfellow16}
\citation{Shrestha19}
\citation{Bengio93}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Regularisation}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Convolutional neural networks}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Convolutional layers}{7}\protected@file@percent }
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Aghdam17}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Pooling layers}{8}\protected@file@percent }
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Shi16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\@writefile{toc}{\contentsline {section}{\numberline {6}Model training set-up}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Universal training configurations}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Image data preparation}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}4-phase classifier models}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Dataset construction}{10}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Dataset distribution for 4-phase classifier models.\relax }}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Model architectures and training configurations}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Architectural diagram of the sequential CNN model with input size $256 \times 256$ and three convolutional layers. The dimensions are displayed as width $\times $ height $\times $ channels for each convolutional and max pooling layer. Not to scale.\relax }}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Results}{12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Plots of the mean accuracies against number of convolutional layers for the 4-phase models over three training runs.\relax }}{12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Test set confusion matrices for the models with highest and lowest overall test accuracies. The value in each square represents the fraction of examples with the true phase label for which the model output was the predicted phase label. We therefore aim for values close to one along the diagonal, which corresponds to correct model outputs.\relax }}{13}\protected@file@percent }
\citation{Dierking03}
\citation{ILSVRC15}
\citation{Szegedy2015}
\citation{Szegedy2015}
\citation{Szegedy2015}
\@writefile{toc}{\contentsline {section}{\numberline {8}Smectic A and C binary classifier models}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Dataset distribution}{14}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Dataset distribution for smectic A and C classifier models.\relax }}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Model architectures and training configuration}{14}\protected@file@percent }
\bibdata{report}
\bibcite{Carleo19}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Architectural diagram of an Inception block. The input is shared between 4 parallel convolution operations, with their outputs being concatenated along the channel axis in the final layer. This results in the output having a number of channels equal to the sum of the number of channels of each concatenated layer. Different kernel sizes in each parallel layer can extract different sized features from the input, propagating more information through the output of the block. The $1 \times 1$ kernel layers preceeding the $3 \times 3$ and $5 \times 5$ ones can be used to reduce the number of channels and, therefore, the computational cost, if necessary. The pooling layer is there to provide some translational invariance \cite  {Szegedy2015}.\relax }}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Results}{15}\protected@file@percent }
\bibcite{Goodfellow16}{2}
\bibcite{LeCun89}{3}
\bibcite{ILSVRC15}{4}
\bibcite{Dierking03}{5}
\bibcite{Sigaki20}{6}
\bibcite{Minor20}{7}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \IeC {\textbullet }\relax }}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}General smectic phase classifier models}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Dataset distribution}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Model architectures and training configuration}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Results}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Conclusions}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Going forward}{16}\protected@file@percent }
\bibcite{Demus99}{8}
\bibcite{Murphy12}{9}
\bibcite{Kaufman12}{10}
\bibcite{Minsky69}{11}
\bibcite{Rumelhart86}{12}
\bibcite{Haykin98}{13}
\bibcite{Glorot11}{14}
\bibcite{Hornik89}{15}
\bibcite{Kline05}{16}
\bibcite{Richard91}{17}
\bibcite{Amari93}{18}
\bibcite{Kingma14}{19}
\bibcite{Bishop95}{20}
\bibcite{Srivastava2014}{21}
\bibcite{Shrestha19}{22}
\bibcite{Bengio93}{23}
\bibcite{Aghdam17}{24}
\bibcite{Shi16}{25}
\bibcite{Szegedy2015}{26}
\@writefile{toc}{\contentsline {section}{\numberline {A}Backpropagation}{18}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Adam optimiser}{18}\protected@file@percent }
