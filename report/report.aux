\relax 
\bibstyle{ieeetr}
\citation{Carleo19}
\citation{Goodfellow16}
\citation{Carleo19}
\citation{Goodfellow16}
\citation{LeCun89}
\citation{Goodfellow16}
\citation{ILSVRC15}
\citation{Dierking03}
\citation{Sigaki20}
\citation{Minor20}
\citation{Sigaki20}
\citation{Demus99}
\citation{Dierking03}
\citation{Demus99}
\citation{Dierking03}
\citation{Demus99}
\citation{Dierking03}
\citation{Demus99}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background theory}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Liquid crystal phases}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Structure}{1}\protected@file@percent }
\citation{Dierking03}
\citation{Dierking03}
\citation{Dierking03}
\citation{Goodfellow16}
\citation{Murphy12}
\citation{Murphy12}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Polarised microscopy}{2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Liquid crystal textures from the dataset, from left to right: nematic phase compound 5CB, cholesteric phase compound D5, and smectic C phase compound M10.\relax }}{2}\protected@file@percent }
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Murphy12}
\citation{Goodfellow16}
\citation{Murphy12}
\citation{Goodfellow16}
\citation{Murphy12}
\citation{Goodfellow16}
\citation{Kaufman12}
\citation{Minsky69}
\citation{Rumelhart86}
\citation{Haykin98}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}General machine learning principles}{3}\protected@file@percent }
\citation{Rumelhart86}
\citation{Haykin98}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Feedforward neural networks}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Forward propagation}{4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Diagram of a feedforward fully-connected neural network with three input values, $D=3$, $W_1=W_2=4$ and one output unit.\relax }}{4}\protected@file@percent }
\citation{Haykin98}
\citation{Glorot11}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Hornik89}
\citation{Haykin98}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Haykin98}
\citation{Goodfellow16}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Training}{5}\protected@file@percent }
\citation{Goodfellow16}
\citation{Kline05}
\citation{Richard91}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Amari93}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Kingma14}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Bishop95}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Srivastava2014}
\citation{Ioffe15}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Regularisation}{7}\protected@file@percent }
\citation{Goodfellow16}
\citation{Shrestha19}
\citation{Bengio93}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Convolutional neural networks}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Convolutional layers}{8}\protected@file@percent }
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Goodfellow16}
\citation{Aghdam17}
\citation{Aghdam17}
\citation{Goodfellow16}
\citation{Shi16}
\citation{Goodfellow16}
\citation{Goodfellow16}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Pooling layers}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Project work}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}First semester goals}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Model training set-up}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Universal training configurations}{9}\protected@file@percent }
\citation{Williams20a}
\citation{Williams20b}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Image data preparation}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}4-phase classifier models}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Dataset construction}{11}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Dataset distribution for 4-phase classifier models.\relax }}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Model architectures and training configurations}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Results}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Plots of the mean accuracies against number of convolutional layers for the 4-phase models over three training runs.\relax }}{12}\protected@file@percent }
\citation{Dierking03}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Smectic A and C binary classifier models}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Dataset construction}{13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Test set confusion matrices for the models with highest and lowest overall test accuracies. The value in each square represents the fraction of examples with the true phase label for which the model output was the predicted phase label. We therefore aim for values close to one along the diagonal, which corresponds to correct model outputs.\relax }}{14}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Dataset distribution for smectic A and C classifier models.\relax }}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Model architectures and training configuration}{14}\protected@file@percent }
\citation{ILSVRC15}
\citation{Szegedy2015}
\citation{Szegedy2015}
\citation{Szegedy2015}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Architectural diagram of an Inception block. The input is shared between 4 parallel convolution operations, with their outputs being concatenated along the channel axis in the final layer. This results in the output having a number of channels equal to the sum of the number of channels of each concatenated layer. Different kernel sizes in each parallel layer can extract different sized features from the input, propagating more information through the output of the block. The $1 \times 1$ kernel layers preceeding the $3 \times 3$ and $5 \times 5$ ones can be used to reduce the number of channels and, therefore, the computational cost, if necessary. The pooling layer is there to provide some translational invariance \cite  {Szegedy2015}.\relax }}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Results}{16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Plots of the mean accuracies over three training runs against number of convolutional layers for sequential models, and number of Inception blocks for Inception models, for the smectic A and C models.\relax }}{16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Test set confusion matrices for the smectic A and C models with highest and lowest overall test accuracies.\relax }}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}General smectic classifier models}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Dataset construction}{17}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Dataset distribution for general smectic classifier models.\relax }}{17}\protected@file@percent }
\bibdata{report}
\bibcite{Carleo19}{1}
\bibcite{Goodfellow16}{2}
\bibcite{LeCun89}{3}
\bibcite{ILSVRC15}{4}
\bibcite{Dierking03}{5}
\bibcite{Sigaki20}{6}
\bibcite{Minor20}{7}
\bibcite{Demus99}{8}
\bibcite{Murphy12}{9}
\bibcite{Kaufman12}{10}
\bibcite{Minsky69}{11}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Model architectures and training configuration}{18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3}Results}{18}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusions}{18}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Going forward}{18}\protected@file@percent }
\bibcite{Rumelhart86}{12}
\bibcite{Haykin98}{13}
\bibcite{Glorot11}{14}
\bibcite{Hornik89}{15}
\bibcite{Kline05}{16}
\bibcite{Richard91}{17}
\bibcite{Amari93}{18}
\bibcite{Kingma14}{19}
\bibcite{Bishop95}{20}
\bibcite{Srivastava2014}{21}
\bibcite{Ioffe15}{22}
\bibcite{Shrestha19}{23}
\bibcite{Bengio93}{24}
\bibcite{Aghdam17}{25}
\bibcite{Shi16}{26}
\bibcite{Williams20a}{27}
\bibcite{Williams20b}{28}
\bibcite{Szegedy2015}{29}
\@writefile{toc}{\contentsline {section}{\numberline {A}Backpropagation algorithm}{20}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Adam optimiser}{20}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Example architecture diagrams}{20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Architectural diagram of the sequential CNN model with input size $256 \times 256$ and three convolutional layers. The output dimensions are displayed as width $\times $ height $\times $ channels for each convolutional and max pooling layer. Not to scale.\relax }}{21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Architectural diagram of the Inception CNN model with one block. The output dimensions are displayed as width $\times $ height $\times $ channels for each convolutional, max pooling and Inception block layer. Not to scale.\relax }}{22}\protected@file@percent }
