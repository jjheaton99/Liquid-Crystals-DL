\documentclass[12pt]{article}
\usepackage[a4paper, total={6.6in, 9.4in}]{geometry}

%\setlength{\parskip}{3pt}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{authblk}
\usepackage{url}
\usepackage{appendix}
\usepackage{booktabs}
\usepackage[font=small]{caption}
\usepackage{subcaption}

\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

\bibliographystyle{ieeetr}

\begin{document}

\title{Multi-phase classification of liquid crystal textures using convolutional neural networks}
\author{\textit{Joshua Heaton}\\\textit{10133722}}
\affil{Department of Physics and Astronomy, The University of Manchester}
\affil{MPhys project report}
\affil{Project performed in collaboration with James Harbon\\Supervisor: Dr Ingo Dierking}
\date{\today}

\maketitle

\begin{abstract}
\end{abstract}

\pagenumbering{gobble}
\newpage
\tableofcontents

\pagenumbering{gobble}
\newpage
\pagenumbering{arabic}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Introduction}
Machine learning (ML) is the term assigned to a wide range of computer algorithms that use data to automatically improve their performance on a specific task. These tasks can take various forms, including decision making, pattern recognition, and prediction  \cite{Murphy12}. A sub-field of ML known as deep learning (DL) generally consists of applying large-scale multi-layer neural networks, a type of algorithm inspired by the structure of the brain, to tasks involving highly complex abstractions of data. Such intensive algorithms typically require vast quantities of data and powerful computational resources to be trained effectively, with the advantage that they do not require any manual feature extraction \cite{Goodfellow16}. With the recent explosion in availability of such data and sophisticated computing technology, DL has seen a surge in interest and application among several fields \cite{Shrestha19}. Computer vision is one such field that has been impacted greatly. Convolutional neural networks (CNNs), a type of neural network suited particularly well to grid-based data, have proven extremely successful in the tasks of image classification, segmentation, and object detection \cite{Voulodimos18}.

There are many thousands of individual documented liquid crystal (LC) compounds, with each displaying a certain sequence of identifiable phases between that of a liquid and solid \cite{Dierking03}. Commonly, polarised microscopy is used to capture images of the textures produced by LC phases for identification by eye \cite{Dierking03}. Literature on machine learning for LC phase classification is sparse, with most studies focusing on the extraction of physical properties of LCs using simulated texture data \cite{Sigaki20, Sigaki19, Minor20, Walters19}, or other means \cite{Florin07, Butnariu13, Doi19, Inokuchi20}. Of most relevance is the work by Sigaki et al., in which they utilise CNNs to classify simulated isotropic and nematic phases to high accuracy \cite{Sigaki20}.

In this project, we prepare a novel dataset of LC texture images captured by polarised microscopy (PM), spanning multiple phases of all orders. Subsequently, we apply CNN classifier models to various phase groupings, probing the limits of attainable model accuracy. The work expands on and consolidates that of the first semester report \cite{Heaton20}, in which we demonstrated the viability of CNNs in some simple LC phase classification tasks. This report will provide a brief overview of LC phases, supervised ML, and CNNs, with further detail found in the first report \cite{Heaton20}. Details of the models used will then be provided, followed by a presentation of the results when they are applied to each of the prepared datasets. Summary conclusions and the limitations of the study will then be discussed.
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Background principles}

\subsection{Liquid crystals}
Liquid crystal phases are characterised by the positional and orientational order of the molecular arrangement. In general, the phase of lyotropic LCs depends on the concentration of the sample in a solvent whilst thermotropic LCs, studied in this project, become more ordered with decreasing temperature. The order and overall structure of the LC phase determines its optical properties, in particular its birefringence. This enables images of the textures of a liquid crystal to be obtained by polarised microscopy, in which the sample is placed between two perpendicularly aligned polarisers. Polarised light incident on the set-up is altered in accordance with the current phase of the LC sample, producing characteristic features in the resulting image.

At sufficiently high temperatures, thermotropic LCs take the form of a fully anisotropic liquid with no structural order and hence birefringence, resulting in completely dark PM textures. Upon cooling, they will display at least one higher ordered phase before reaching the fully crystalline stage. Of lowest order, just orientational, is the nematic (N) phase, in which the molecules are aligned along a particular axis, called the director, and are still free to move around as in a liquid. Compounds with chiral molecules may instead display the cholesteric (N*) phase, which is the same as the nematic phase except with helical variation of the director. Layered positional order is introduced in the smectic phase, which is divided into three distinct phase groupings. The orientation of the director further categorises these groupings. The fluid smectic (FSm) phases have no positional order within molecular layers. The orientation of the director with respect to the layer planes determines whether the phase is smectic A (SmA), in which it is perpendicular, or C (SmC) otherwise. The smectic B (SmB), I (SmI), and F (SmF) phases are placed into the hexatic smectic (HSm) group, with short-range hexagonal structures generating positional order within the layers. The soft crystal phases differ in that the layers show long-range positional order.

This project uses CNNs to classify PM textures from thermotropic chiral LC compounds, including the phases N*, SmA, SmC, SmI, and SmF.  

\subsection{Supervised machine learning}
Machine learning algorithms can in general be categorised as supervised, unsupervised, or reinforcement learning. The ML implementations of this project are purely supervised learning algorithms. In this case, the ML model is defined as a function, parametrised by learned values $\bm{\theta}$, that maps an input data sample $\bm{x}$ containing various features to an output predicted label $\hat{\bm{y}}$,
\begin{equation}
\hat{\bm{y}}=f(\bm{x};\bm{\theta}). \label{supmodel} 
\end{equation}
$\hat{\bm{y}}$ can take various forms depending on the specific task, for example regression, in which the model attempts to predict a continuous value given the input data, or classification, in which it predicts a category that the input data sample belongs to. A supervised model attempts to learn appropriate $\bm{\theta}$ values for the mapping using a set of training data, consisting of pairs, $i$, of input examples and their corresponding true labels, $\lbrace\bm{x}^{(i)},\bm{y}^{(i)}\rbrace$. The model takes sample $\bm{x}^{(i)}$ and produces an output label prediction $\hat{\bm{y}}^{(i)}$ according to Equation \ref{supmodel}. A chosen cost function $J(\bm{y},\hat{\bm{y}};\bm{\theta})$ is then evaluated, which generally provides a measure of the divergence of the model outputs from the true labels. The parameters are then updated with a particular optimisation algorithm in order to minimise the cost. Successful training as such results in a model that is effective in producing accurate predictions for new unseen data samples. Fixed parameters that define the precise form of $f$, as well as training configurations, are known as hyperparameters.

When deciding on the form of a supervised model, of great importance is its capacity, which can be thought of as the size of the model. A low capacity model with few trainable parameters may not be able to extract or infer enough features from the training data to form good predictions, known as underfitting. On the other hand, too high a capacity will cause the model to learn meaningless or overly-fine features from the training data and it may not perform well when inferring on new unseen examples. This is referred to as overfitting. The model's hyperparameters must, therefore, be properly tuned to ensure it does not over or under fit. In addition to limiting model capacity, regularisation techniques can be applied to help prevent overfitting and improve generalisation.

Measurement of a supervised model's performance is critical in determining how well it will generalise to new unseen data, by evaluating a metric on a dataset of samples. As an example, for classification tasks a common metric is the percentage of samples that the model assigned to the correct class. Often, the training dataset is split into three subsets: training, validation, and test. The training set contains the data samples used to update the trainable parameters of the model. The validation set is used to tune the hyperparameters of the model. Evaluation of a trained model on the training and validation sets can reveal if the model has underfitted or overfitted. In the former case a low performance on both sets will be observed, whereas for the latter a high performance on the training set and low on the validation set will occur. Hyperparameters can then be adjusted accordingly before retraining the model. After a satisfactory model configuration and validation performance are reached, it is evaluated on the as-of-yet unseen test set to give an indication of the model's generalisation error.

\subsection{Convolutional neural networks}
Neural networks are a type of ML algorithm that pass input data through a series of layers, each containing a number of units. Every unit of a layer is connected in a particular way to the units of the previous layer. Units can take various forms including ones with or without trainable parameters, and specific layers are engineered so as to identify, manipulate, and propagate data features from the input to the output of the network. Perhaps the most simple type, in a dense layer the input to each unit is the outputs of all units of the previous layer. The inputs are multiplied by the unit's learned weight parameters and summed together with a learned bias parameter to calculate the unit's output. An activation function can then be applied to the output of each unit of the layer to introduce non-linearity in the network. Such non-linearities are essential in allowing neural networks to act as a universal function approximator, allowing it to perform highly complex inference on input data. A dense layer can hence be represented in matrix form as
\begin{equation}
\bm{O}=A(\bm{W}\bm{I}+\bm{B}),
\end{equation}
where $\bm{O}$ is the vector containing the outputs for the layer, $\bm{W}$ is the matrix of layer weights, $\bm{I}$ is the vector of layer inputs, $\bm{B}$ is the vector of bias parameters, and $A$ is the activation function applied element-wise.



Convolutional neural networks are a class of neural network particularly well suited to handling image data.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Summary of previous work}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{CNN Models}

\subsection{Sequential}

\subsection{Inception}

\subsection{ResNet}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Methodology}

\subsection{Data preparation}

\subsection{Model training configurations}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Classification tasks and results}

\subsection{Cholesteric and smectic}

\subsection{Smectic A and C}

\subsection{Smectic I and F}

\subsection{Cholesteric, fluid smectic and hexatic smectic}

\subsection{Cholesteric, smectic A, C, and hexatic smectic}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Conclusions}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\bibliography{report2}

\appendix
\appendixpage

\end{document}