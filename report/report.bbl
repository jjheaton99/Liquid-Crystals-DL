\begin{thebibliography}{10}

\bibitem{Carleo19}
G.~Carleo {\em et~al.}, ``Machine learning and the physical sciences,'' {\em
  Rev. Mod. Phys}, vol.~91, p.~045002, 2019.

\bibitem{Goodfellow16}
I.~Goodfellow, Y.~Bengio, and A.~Courville, {\em Deep Learning}.
\newblock MIT Press, 2016.

\bibitem{LeCun89}
Y.~LeCun {\em et~al.}, ``Backpropagation applied to handwritten zip code
  recognition,'' {\em Neural Computation}, vol.~1, no.~4, pp.~541--551, 1989.

\bibitem{ILSVRC15}
O.~Russakovsky {\em et~al.}, ``Imagenet large scale visual recognition
  challenge,'' {\em International Journal of Computer Vision (IJCV)}, vol.~115,
  no.~3, pp.~211--252, 2015.

\bibitem{Dierking03}
I.~Dierking, {\em Textures of Liquid Crystals}.
\newblock WILEY-VCH, 2003.

\bibitem{Sigaki20}
H.~Y.~D. Sigaki {\em et~al.}, ``Learning physical properties of liquid crystals
  with deep convolutional neural networks,'' {\em Scientific Reports}, vol.~10,
  p.~7664, 2020.

\bibitem{Minor20}
E.~N. Minor {\em et~al.}, ``End-to-end machine learning for experimental
  physics: using simulated data to train a neural network for object detection
  in video microscopy,'' {\em Soft Matter}, vol.~16, p.~1751, 2020.

\bibitem{Demus99}
D.~Demus {\em et~al.}, {\em Physical Properties of Liquid Crystals}.
\newblock WILEY-VCH, 1999.

\bibitem{Murphy12}
K.~Murphy, {\em Machine Learning: A Probabilistic Perspective}.
\newblock MIT Press, 2012.

\bibitem{Minsky69}
M.~Minsky and S.~A. Papert, {\em Perceptrons: An Introduction to Computational
  Geometry}.
\newblock MIT Press, 1969.

\bibitem{Rumelhart86}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams, {\em Learning Internal
  Representations by Error Propagation}.
\newblock MIT Press, 1986.

\bibitem{Haykin98}
S.~Haykin, {\em Neural Networks: A Comprehensive Foundation}.
\newblock 2~ed., 1998.

\bibitem{Glorot11}
X.~Glorot, A.~Bordes, and Y.~Bengio, ``Deep sparse rectifier neural networks,''
  {\em Proc. Mach. Learn. Res}, vol.~15, pp.~315--323, 2011.

\bibitem{Hornik89}
K.~Hornik, M.~Stinchcombe, and H.~White, ``Multilayer feedforward networks are
  universal approximators,'' {\em Neural Networks}, vol.~2, pp.~359--366, 1989.

\bibitem{Kline05}
D.~Kline and V.~Berardi, ``Revisiting squared-error and cross-entropy functions
  for training neural network classifiers,'' {\em Neur. Comp. App.}, vol.~14,
  pp.~310--318, 2005.

\bibitem{Richard91}
M.~D. Richard and R.~Lippmann, ``Neural network classifiers estimate bayesian a
  posteriori probabilities,'' {\em Neural Computation}, vol.~3, pp.~461--483,
  1991.

\bibitem{Amari93}
S.~ichi Amari, ``Backpropagation and stochastic gradient descent method,'' {\em
  Neurocomputing}, vol.~5, pp.~185--196, 1993.

\bibitem{Kingma14}
D.~Kingma and J.~Ba, ``Adam: A method for stochastic optimization,'' {\em
  International Conference on Learning Representations}, 2014.

\bibitem{Bishop95}
C.~Bishop, ``Regularization and complexity control in feed-forward networks,''
  in {\em Proceedings International Conference on Artificial Neural Networks
  ICANN'95}, vol.~1, pp.~141--148, EC2 et Cie, 1995.

\bibitem{Srivastava2014}
N.~Srivastava {\em et~al.}, ``Dropout: a simple way to prevent neural networks
  from overfitting,'' {\em J. Mach. Learn. Res.}, vol.~15, pp.~1929--1958,
  2014.

\end{thebibliography}
