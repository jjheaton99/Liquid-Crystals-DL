\documentclass[12pt]{article}
\usepackage[a4paper, total={6.6in, 9in}]{geometry}

%\setlength{\parskip}{3pt}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{authblk}
\usepackage{url}
\usepackage{appendix}
\usepackage{booktabs}
\usepackage[font=small]{caption}
\usepackage{subcaption}

\bibliographystyle{ieeetr}

\begin{document}

\title{Can liquid crystal phases be identified via machine learning?}
\author{Joshua Heaton 10133722}
\affil{School of Physics and Astronomy, The University of Manchester}
\affil{MPhys project report}
\date{\today}

\maketitle

\begin{abstract}
hi
\end{abstract}

\pagenumbering{gobble}
\newpage
\pagenumbering{arabic}

%=========================================================================================
\section{Introduction}
Machine learning methods have seen widespread utilisation across all scientific disciplines, in situations where conventional algorithms are too cumbersome to implement for specific data-based and modelling tasks \cite{Carleo19}. Deep learning, loosely defined as machine learning with large datasets, parallel computation and scalable algorithms with many layers \cite{Goodfellow16}, has and continues to increase the range and complexity of possible applications of machine learning in the sciences \cite{Carleo19}. Any task applying deep learning to data with a grid-like form, such as images, likely involves the usage of convolutional neural network (CNN) algorithms \cite{Goodfellow16}. CNNs were conceived in 1989 by Yann LeCun \textit{et al.} and successfully applied to recognition of handwritten characters \cite{LeCun89}. However, their astounding performance in the field of computer vision would not be fully realised until after breakthroughs in deep learning starting in 2006 \cite{Goodfellow16}. Their efficacy was further proven when Geoffrey Hinton \textit{et al.} entered a CNN into the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, and won by a large margin \cite{ILSVRC15}.

Liquid crystal phases are in general identified by eye, directly from textures taken by polarised microscopy. Without adequate experience, this can prove a difficult task because certain unique liquid crystal phases, generated by often minor changes in structural properties, can have similar textural appearances \cite{Dierking03}. Our project aims to test the viability of machine learning algorithms as tools to assist phase identification. CNNs are particularly suitable due to their prevalence in image classification, and so form the core of our investigations. Current literature in this specific topic is limited, and the approaches so far have mostly involved the usage of simulated textures in the training of models \cite{Sigaki20, Minor20}. Sigaki et al. have demonstrated the viability of CNNs in isotropic and nematic phase texture classification and in the prediction of physical liquid crystal properties \cite{Sigaki20}. Our study further explores and attempts to push the limits of the classification task across a wider range of phases, utilising real experimental data produced by polarised microscopy.

This project report will first provide a brief overview of the physics behind liquid crystals and the capturing of their textures by polarised microscopy, as well as an introduction to machine learning, neural networks and CNNs. The details and results of our investigations into phase classification will then be presented, as well as an outlook to further study.
%=========================================================================================
\section{Liquid crystal phases}
Liquid crystals are substances in a state between that of a fully isotropic liquid and a crystal with a periodic lattice structure \cite{Demus99, Dierking03}. The molecules can have varying positional order, and have orientational order over large sections. The unit vector parallel to the alignment of the molecules is called the director \cite{Demus99, Dierking03}. Other details such as molecular shape and chirality affect the overall structure. These variations in structure result in numerous individual identifiable liquid crystal phases \cite{Demus99, Dierking03}. Thermotropic liquid crystals exhibit phases transitions with changing temperature, whereas lyotropic liquid crystals are dissolved in a solvent with the phase depending on the concentration \cite{Demus99}. This project will be concerned with only thermotropic liquid crystals. 

When cooling a thermotropic liquid crystal starting as an isotropic (Iso) liquid, it will first transition to the nematic phase (N), which has orientational order only. The chiral nematic (cholesteric, N*), phase also has no positional order, and has a periodic variation of the director, resulting in helical structures. Upon further cooling, the smectic (Sm) phase will be reached. This can be split into three categories, going from fluid smectic to hexatic smectic to soft crystal in order of decreasing temperature. The fluid smectic phase has molecules arranged in layers, with no positional order in the plane of each layer. When the director is perpendicular to the layer planes, the phase is smectic A (SmA), with smectic C (SmC) having a director that is tilted by comparison. Hexatic smectic phases have short range positional order within the layer planes with hexagonal intermolecular arrangements interspersed with order-breaking defects. This encompasses the smectic B (SmB), I (SmI), and F (SmF) phases. Semctic B has a director perpendicular to the layer planes, whereas it is tilted towards the vertices of the hexagons for smectic I and towards to the sides of the hexagons for smectic F. The soft crystal phases are defect free within the layers and therefore exhibit long range positional order \cite{Dierking03}.

The liquid crystal texture data used in this project have all been obtained by polarised microscopy captured with a video camera. In brief terms, a polarising microscope works by placing a sample between perpendicularly aligned polarisers. When light is shone through the arrangement the resulting image will be dark, unless the sample rotates the plane of polarisation \cite{Dierking03}. In the case of liquid crystals, the isotropic liquid phase has no optical properties so will produce completely dark textures. The nematic, cholesteric and smectic phases are anisotropic and therefore birefringent, with optical axes depending on their structures. This produces unique textural image features for each phase \cite{Dierking03}. Some example textures taken from our dataset are displayed in Figure 1.

\begin{figure}[h]
\centering
\includegraphics[width=6in]{images/texture_samples.png}
\caption{Liquid crystal textures from the dataset, from left to right: nematic phase compound 5CB, cholesteric phase compound D5, and smectic C phase compound M10.}
\end{figure}
%=========================================================================================
\section{General machine learning principles}
A machine learning model is a computer algorithm which automatically improves its performance in a given task as it gains experience from a dataset \cite{Goodfellow16}. It learns patterns in data and uses these patterns to make probabilistic predictions. The data normally takes the form of a set of $N$ examples, which are usually expressed as vectors, or some other structure of features, $\{\bm{x}^{(i)}\}_{i=1}^N$, containing quantitative information about example $i$ \cite{Murphy12}. In supervised learning the examples are given labels $y^{(i)}$, to form a training set of pairs $\{\bm{x}^{(i)}, y^{(i)}\}_{i=1}^N$, and the model attempts to learn the mapping from a general input $\bm{x}$ to an output $\hat{y}$. One main type of supervised learning is regression, in which the output is a numerical scalar. The other main type is classification, in which the model predicts what the input belongs to out of a selection of classes. In unsupervised learning there are no labels, and the algorithm attempts to learn specific patterns in the dataset such as clusters of similar data points \cite{Murphy12}. The topic of this project is a supervised classification problem.

A supervised model can be usually be expressed as a function of inputs and a set of parameters $\boldsymbol\theta$ such that $\hat{y}=f(\bm{x};\boldsymbol\theta)$. Training involves optimisation of the parameters by minimisation of a cost function $J(\boldsymbol\theta)$, which measures the deviation of the predictions of the model from the true labels. The most common optimisation algorithms involve computing the gradient of $J(\boldsymbol\theta)$ with respect to $\boldsymbol\theta$ \cite{Goodfellow16}. 

The capacity of a model is akin to its complexity. The number of trainable parameters can give a fast indication of capacity. However, it also depends on the model's functional form. The parameters controlling the capacity of the model, as well as certain other training settings, are known as hyperparameters \cite{Goodfellow16}. If the capacity is too small, the model will tend to "underfit" the training set, resulting in poor performance even when optimised well. On the other hand, too high a capacity will result in "overfitting", with high performance on the training set, but the model may have a high generalisation error, which is the model's error rate when evaluating it on new, unseen data \cite{Murphy12, Goodfellow16}. Before training begins, the entire dataset is often split into training, validation, and test sets, containing $N_{\mathrm{train}}$, $N_{\mathrm{valid}}$ and $N_{\mathrm{test}}$ examples respectively. The training set, as defined previously, is used to optimise the parameters. The model's performance is then evaluated on the validation set. A poor performance on both the training and validation sets is indicative of underfitting, whereas a high performance on the training set and low on the validation set suggests overfitting. The validation set can therefore be used to tune the hyperparameters of the model before retraining. This can be repeated until the model fits optimally \cite{Murphy12, Goodfellow16}. The final model is then evaluated on the as-of-yet unseen test set to provide an estimate of its generalisation error \cite{Murphy12}. Methods used to reduce generalisation error, such as reducing model capacity, are known as regularisation. \cite{Goodfellow16}.

Data leakage, in the case of supervised learning, is when there are examples in the validation or test sets with a high degree of similarity to those in the training set. The model will easily produce the correct output when evaluated on the leaked examples, especially when overfitting has occurred. This can result in a false indication of low generalisation error. Therefore, data leakage must be avoided in order to produce a reliable model \cite{Kaufman12}.
%=========================================================================================
\section{Feedforward neural networks}
\subsection{Forward propagation}
A neural network is a type of machine learning model that takes inspiration from the current understanding of how the brain works \cite{Minsky69}. The inputs are forward propagated through a series of connected hidden units, akin to neurons in a brain, before reaching the output units. In the most basic form, a fully connected neural network, The units are arranged into layers, with each unit in a layer connected to every unit of the previous layer \cite{Rumelhart86}. We will define the total number of layers, excluding the input layer, as the depth, $D$, of the model, with the width, $W_l$, of layer $l\in[0..D]$ equal to the number of units it contains. $l=0$ is the input layer. The choice of the hyperparameters $D$ and $W_l$ defines the architecture of the model \cite{Haykin98}. A schematic of an example network is presented in Figure 2. 

\begin{figure}[h]
\centering
\includegraphics[width=3.5in]{images/feedforward_nn.png}
\caption{Diagram of a feedforward fully-connected neural network with three input values, $D=3$, $W_1=W_2=4$ and one output unit.}
\end{figure}

A single hidden unit's output value is calculated by multiplying the output of each of the previous layer's units with a weight parameter, summing these together with a bias parameter, and then passing the result through a non-linear activation function \cite{Rumelhart86}. More formally, the output $O_{lu}$ of hidden unit $u\in[1..W_l]$, of layer $l$, with bias parameter $b_{lu}$ and activation function $A(h)$ is calculated as
\begin{equation}
O_{lu}=A(h_{lu})
\end{equation}
with
\begin{equation}
h_{lu}=b_{lu}+\sum_{v=1}^{W_{l-1}}\theta_{luv}O_{(l-1)v}
\end{equation}
for $l > 0$. $\theta_{luv}$ is the weight parameter that hidden unit $u$ of layer $l$ applies to the output of unit $v\in[1..W_{l-1}]$ of the previous layer \cite{Haykin98}. $O_{0u}$, is equivalent to value $x_u$ of the the input vector $\bm{x}$. In matrix form,
\begin{equation}
\bm{O}_l=A(\bm{h}_l)
\end{equation}
with
\begin{equation}
\bm{h}_l=\boldsymbol\theta_l\bm{O}_{l-1}+\bm{b}_l
\end{equation}
where $\boldsymbol\theta_l$ is the $W_l \times \left(W_{l-1}\right)$ dimensional matrix of weights for layer $l$, with rows corresponding to the weights of each hidden unit, $\bm{O}_l$ is the vector of outputs for layer $l$, and $\bm{b}_l$ is the vector of bias parameters for layer $l$. For the first layer,
\begin{equation}
\bm{h}_1=\boldsymbol\theta_1\bm{x}+\bm{b_1}\; \cite{Haykin98}.
\end{equation}
The activation function is applied element-wise. A highly effective choice of activation function for hidden units, that performs a similar operation to biological neurons, is the rectified linear unit (ReLU),
\begin{equation}
A(h)=\mathrm{max}(0,h)\; \cite{Glorot11}.
\end{equation}

The number of units in the final output layer depends on the type of model. For regression, there will be one unit that outputs a continuous-valued prediction \cite{Goodfellow16}. For classification, the number of final layer units is equal to the number of classes, $C=W_D$, with each one outputting the predicted probability that an input $\bm{x}$ belongs to a particular class. The network is therefore summarised as $\hat{\bm{y}}=f(\bm{x};\boldsymbol\theta)$, with $\hat{\bm{y}}$ being the $C$ dimensional vector of output probabilities. Each example in the dataset is labelled by a vector $\bm{y}$ with a value of one for the component corresponding to the true class, and zero for all other components. The component $y_u$, with $u\in[1..C]$, is equivalent to the Kronecker delta $\delta_{ut}$ where the index $t$ corresponds to the true class \cite{Goodfellow16}. The most common choice of final layer activation function for classification is the softmax function, $\sigma_{SM}(h)$, in which case the components $\hat{y}_u$ of the output probability prediction vector, are given by
\begin{equation}
\hat{y}_u=O_{Du}=\frac{e^{h_{Du}}}{\sum_{v=1}^{C}e^{h_{Dv}}}\;\cite{Goodfellow16}.
\end{equation} 
When there are just two classes, known as binary classification, only one unit is needed in the output layer. The data labels $y$ are equal to one or zero depending on the class that $\bm{x}$ belongs to \cite{Goodfellow16}. The activation function in this case is generally the logistic sigmoid function, $\sigma(h)$, with
\begin{equation}
\hat{y}=O_D=\frac{1}{1+e^{-h_D}}
\end{equation}
where in this situation $\boldsymbol\theta_D$ is a $W_{D-1}$ dimensional vector. The value of $\hat{y}$ is the predicted probability that $\bm{x}$ belongs to one of the classes, with a probability of $1-\hat{y}$ that it belongs to the other \cite{Goodfellow16}. 

In 1989 Kurt Hornik \textit{et al.} mathematically proved that feedforward neural networks with multiple layers and non-linear activations can approximate any continuous function given the correct configuration \cite{Hornik89}. Another advantage of neural networks is that they can automatically learn to extract useful higher-level features from the raw input data \cite{Haykin98}.

\subsection{Training}
Neural network training starts with random initialisation of the weights, for example by drawing values from a normal distribution \cite{Goodfellow16}. A single update step is generally carried out by calculating the model output for each example, followed by the gradient of the cost function with respect to the model weight parameters, $\bm{g}=\bm\nabla_{\bm\theta}J(\bm\theta)$. An iterative optimisation algorithm then uses the gradient to update the parameters to reduce the cost \cite{Goodfellow16}. $\bm{g}$ is obtained using the backpropagation algorithm, which calculates the gradient of $J(\bm\theta)$ with respect to the final outputs, and then recursively applies the chain rule going backwards through the network, calculating the derivatives with respect to the outputs of each hidden unit followed by their parameters \cite{Haykin98}. Backpropagation is detailed in Appendix I. The overall goal of training iterations is to reduce the generalisation error of the model \cite{Goodfellow16}. Generally, the cost function is the expectation value of the loss of all examples in the training set,
\begin{equation}
J(\boldsymbol\theta)=\frac{1}{N_{\mathrm{train}}}\sum_{i=1}^{N_{\mathrm{train}}}L(f(\bm{x}^{(i)};\boldsymbol\theta),\bm{y}^{(i)})
\end{equation}
where the loss is the cross-entropy between the model's output probabilities and the input's true class label,
\begin{equation}
L(f(\bm{x};\boldsymbol\theta),\bm{y})=L(\hat{\bm{y}},\bm{y})=-\sum_{u=1}^Cy_u\mathrm{log}(\hat{y}_u)\; \cite{Goodfellow16}.
\end{equation}
When training a classifier model, the softmax activation function of the final layer can be included in the loss function, to give the categorical cross-entropy,
\begin{equation}
L_{CCE}(\hat{\bm{y}},\bm{y})=-\sum_{u=1}^Cy_u\mathrm{log}\,\sigma_{SM}(h_{Du})=-\mathrm{log}\left(\frac{e^{h_{Dt}}}{\sum_{j=1}^{C}e^{h_{Dj}}}\right)
\end{equation}
where we have used the fact that $y_u=\delta_{ut}$ for classification \cite{Kline05}. For binary classification in which the logistic sigmoid activation function is used, the binary cross-entropy is
\begin{equation}
L_{BCE}(\hat{y},y)=
\left\{
\begin{array}{ll}
-\mathrm{log}\,\sigma(h_D) & \mbox{if } y^{(\mathrm{true})}=1 \\
-\mathrm{log}\left(1-\sigma(h_D)\right) & \mbox{if } y^{(\mathrm{true})}=0
\end{array}
\right.\;\cite{Richard91}.
\end{equation}

Calculating the exact derivative of the cost function is extremely computationally expensive in most situations when $N_{\mathrm{train}}$ is large. Instead, an approximation of the gradient, $\hat{\bm{g}}$, is calculated by randomly sampling a small batch of $m$ examples from the training data, giving
\begin{equation}
\hat{\bm{g}}=\frac{1}{m}\bm\nabla_{\bm\theta}\sum_{i=1}^mL(f(\bm{x}^{(i)};\boldsymbol\theta),\bm{y}^{(i)})\;\cite{Goodfellow16}.
\end{equation}
In general the performance loss from the difference between $\hat{\bm{g}}$ and the exact gradient $\bm{g}$ is outweighed by the greatly decreased training step computation time \cite{Goodfellow16}. Optimisation methods that use this random batch sampling are known as stochastic methods. Typically parameter update steps are performed batch by batch, with no duplicate example selections, until the whole training set has been seen by the model. After this all examples are again available for selection. Such a cycle is known as an epoch of training, with $\mathrm{floor}(N_{\mathrm{train}}/m)$ update steps \cite{Goodfellow16}.

Stochastic gradient descent (SGD) is a basic batch-based optimisation algorithm in which the parameters are updated in the opposite direction of $\hat{\bm{g}}$,
\begin{equation}
\bm\theta\leftarrow\bm\theta-\epsilon\hat{\bm{g}}
\end{equation}
where $\epsilon$ is a hyperparameter called the learning rate, which controls the amount by which the parameters change with each update \cite{Amari93}. $\epsilon$ must be chosen carefully because it greatly affects the training stability and duration \cite{Goodfellow16}. For large neural networks the cost function has a highly irregular multi-dimensional form with many local minima, meaning that in general the final model will not find the global minimum. However, a local minimum is often enough to achieve low generalisation error \cite{Goodfellow16}. For all models trained in this project, the Adam optimisation algorithm was used, detailed in Appendix II. This is a type of SGD in which the learning rate at each step is adjusted based on unbiased estimates of the first and second moments of the gradient. This algorithm is less prone to becoming stuck in shallow local minima and is not too sensitive to the choice of its hyperparameters \cite{Kingma14}. 

\subsection{Regularisation}
Neural networks can be prone to overfitting, especially ones with a high capacity or where the dataset is small. Good regularisation is therefore a requirement for models to perform well on the test set \cite{Goodfellow16}. There are a variety of strategies, from which the key ones used in this project are dataset augmentation, early stopping, dropout, and batch normalisation.

Augmentation involves adding extra examples to the training set that are altered versions of the originals, increasing the effective overall number of training examples and improving generalisation. The same effect can also be achieved by performing specific random transformations on each example as they are selected for each training batch. For example, image data can undergo random rotations, reflections, translations, magnification and other transformations. Of course, this must produces images that could still feasibly be a member of the original dataset \cite{Goodfellow16}.

If a model is trained for too many epochs, the performance on the training set may still be improving, but the generalisation error will start to increase due to overfitting \cite{Bishop95, Goodfellow16}. Early stopping aims to prevent this. A model performance measure, typically the cost function evaluated on a random batch from the validation set, is monitored during training. If after a specified number of epochs the performance has not improved by more than a tolerance value, the training will be stopped. The number of epochs allowed for improvement is called the patience hyperparameter \cite{Goodfellow16}.

When using dropout, at each training step hidden units are randomly chosen to not be included in the step, by multiplying their output by zero. The probability for a unit with dropout to not be included is equal to the dropout rate hyperparameter, which is often set to $\frac{1}{2}$. When applied applied to certain individual layers or to all hidden units, dropout simulates the training of many sub-models that all share parameters, encouraging each hidden unit to learn more general and useful features. This improves regularisation by increasing the final model's robustness to noise \cite{Srivastava2014}.

Batch normalisation

%=========================================================================================
\section{Convolutional neural networks}
\subsection{Convolutional layers}
CNNs are a type of feedforward neural network in which at least one layer uses the convolution operation to propagate information from the previous layer, instead of the standard fully-connected configuration \cite{Goodfellow16}. They have proven to be extremely effective at processing inputs with a large number of grid-like features, in particular image data \cite{Shrestha19}. CNNs are trained in the same way as standard fully connected neural networks, with slight modifications to the backpropagation algorithm \cite{Bengio93}.

For image-based CNNs, a convolutional layer takes an input tensor $\bm{I}$ with width $W_I$, height $H_I$ and depth $D_I$, where the depth is the number of channels. Grayscale images have one channel, with a value corresponding to the brightness of the pixel at that location, whereas colour images have three channels, corresponding to the red, green and blue values for the pixel \cite{Aghdam17, Goodfellow16}. The input is convolved with $N_K$ kernels, which are tensors $\bm{K}^{(d)}$ with width $W_K<W_I$, height $H_K<H_I$ and depth $D_K=D_I$. This produces a pre-activation output tensor $\bm{\bm{h}}$, which in the most basic case has width $W_h=W_I$, height $H_h=H_I$ and depth $D_h=N_KD_I$. $\bm{I}$ is convolved with each $\bm{K}^{(l)}$ in turn, with each output stacked together depth-wise to give the total output $\bm{h}$. Having more than one kernel increases the number of channels in the next layer. This overall operation is given by
\begin{equation}
h_{ijk}=\sum_{m=1}^{H_K}\sum_{n=1}^{W_K}I_{(i-m)(j-n)r}K_{mnr}^{(d)}
\end{equation}
with $r=k\,\mathrm{mod}\,D_I$ and $d=\lceil\frac{k}{D_I}\rceil$ \cite{Aghdam17, Goodfellow16}. Similarly to standard fully connected, or dense, hidden layers, a non-linear activation function is applied to every element of $\bm{h}$ \cite{Aghdam17, Goodfellow16}. 

Using more than one kernel allows a convolutional layer to extract more features from its input, at the cost of increased computation time and memory usage \cite{Goodfellow16}. The trainable parameters are the elements of the kernels. The kernels can individually learn different features to extract from the input, with the size of the features being related to the height and width dimensions of the kernels \cite{Aghdam17, Goodfellow16}. The usage of kernels in convolutional layers is a form of parameter sharing, which is when parameters are used for more than one operation in a model. Compared to a dense layer, many less parameters are needed, resulting in greatly reduced memory consumption, and often improved regularisation \cite{Aghdam17, Goodfellow16}. 

At the edges of the input, we have to consider the type of padding to use. Valid padding is when the kernel is kept completely within the bounds of the input, which results in the output having a smaller width and height than the input depending on the kernel size \cite{Aghdam17, Goodfellow16}. The case we have discussed in which the width and height of the output are equal to the input corresponds to same padding, where each pixel is visited by the kernel the same number of times, resulting in it overlapping the boundary of the input at the furthest points. Any kernel parameters outside the input region are multiplied by zero \cite{Aghdam17, Goodfellow16}.

Another adjustable property of convolutional layers is the stride. This is the number of elements by which the kernel is moved with each step. The stride can be different for the horizontal and vertical directions. The basic case of Equation 15 corresponds to a stride of one in both directions. Strides greater than one result in dimensionality reduction from input to output, with less feature extraction. However, this reduces computational cost for the layer \cite{Aghdam17, Goodfellow16}.

\subsection{Pooling layers}
Convolutional layers are often directly followed by pooling. A pooling layer takes rectangles of a specific size, called the pool size, from the output of the previous layer. It outputs single values as a function of the values in each rectangle, similarly to how kernels work in convolutional layers \cite{Aghdam17, Goodfellow16}. However, the function used by a pooling layer does not contain any trainable parameters, and each channel is processed individually \cite{Aghdam17, Goodfellow16}. Padding and stride are also defined for pooling layers, in the same way as for convolutional layers. As an example, a pooling layer with pool size and strides of $2 \times 2$, valid padding, and input with dimensions $10 \times 10 \times 3$ will produce an output with dimensions $5 \times 5 \times 3$ \cite{Aghdam17, Goodfellow16}. There are a variety of pooling functions, such as max pooling, where the outputs are the maximum values in each rectangle, and average pooling, where the outputs are the means of the rectangles \cite{Goodfellow16}. Global pooling layers are a special case with a pool size equal to the width and height of the input, resulting in one output value for each of the input channels. The output is therefore a vector, suitable to be fed into a dense layer \cite{Aghdam17}.

Pooling layers are utilised for two key reasons. Firstly, they can reduce the dimensions of the network between convolutional layers, decreasing computational cost. Secondly, they can make the CNN partially invariant to translations of features in the input \cite{Aghdam17, Goodfellow16}.
%=========================================================================================
\section{Model training set-up}
\subsection{Universal training configurations}
All CNN models in this project are implemented using the TensorFlow machine learning library with Keras high-level API, and trained using NVIDIA CUDA with cuDNN on an NVIDIA RTX 2060 graphics processing unit (GPU). GPUs are typically utilised for training deep models due to their ability to perform thousands of tensor calculations in parallel, resulting in greatly decreased model training time compared with using a standard CPU \cite{Shi16, Goodfellow16}. Any hyperparameters corresponding to model tensor dimensions, specifically batch size, image input size, and model layer dimensions have been chosen as multiples of powers of two. This results in more efficient usage of GPU memory \cite{Goodfellow16}. 

For every model, a batch size of $m=32$, the Adam optimiser with hyperparameters $\epsilon=0.001$, $\beta_1=0.9$, $\beta_2=0.999$ and $\delta=10^{-7}$ and early stopping  monitored by validation set loss and with a patience of 100 epochs is used. After early stopping, the parameters with the lowest validation loss during training were saved as the final model. Every convolutional or dense hidden layer in all models has ReLU activations and batch normalisation. Every convolutional layer has strides of $1 \times 1$, and every dense layer has dropout with rate $\frac{1}{2}$.

We will define sequential neural network model architectures as models without branching layers, and branching architectures as the opposite, in which the outputs of certain layers are shared between multiple subsequent layers. Also, when referring to kernel size, we quote only the width by the height, with the depth inferred from the number of channels in the layer. We will refer to max pooling with pool size and strides of $2 \times 2$ and same padding as a standard pooling layer.

Dataset augmentations are implemented automatically during training using the image data generator function in Keras. Batches of images are loaded from the training directory, and specified transformations are applied randomly to each image before being used for training. The original image remains unchanged in the root directory. We define flip augmentations as random flipping of images vertically and horizontally, and all augmentations as flip augmentations along with random rotations by up to $30^\circ$, image width and height shifting by up to $10\%$, and uniform rescaling by up to $20\%$.

The accuracy metric we use to test our models is the percentage of correct outputs when a model is evaluated on a specific dataset.
\subsection{Image data preparation}
We obtain liquid crystal texture image data by extracting images frame by frame from polarised microscopy videos, using the VLC Media Player software. A majority of the videos display more than one liquid crystal phase. Phase labels are assigned to each image based on the phase the video was displaying at the time the image was extracted. Images extracted at the point of a phase transition are either labelled with the current dominant phase or discarded if it is unclear. Depending on the pixel dimensions of the images, they are further split into smaller images with the same phase labels. Excess pixels are then cropped so that the images are square, with dimensions larger than the model input dimensions, which are square for every model. The images are then resized to the input dimensions of the model, and converted from three-channel RGB colour mode to one-channel grayscale, with pixel brightness values in the range $[0,1]$. We have made the assumption that liquid crystal texture identification is independent of colour.

Texture images of the same phase from the same video can be very similar. Therefore, in order to prevent data leakage, these are not shared between any of the training, validation or test sets for a specific model. Conversion to grayscale could also help to prevent data leakage because different videos of the same liquid crystal compound have similar colours.
%=========================================================================================
\section{4-phase classifier models}
\subsection{Dataset construction}
The first set of models were created to test the viability of CNNs in the liquid crystal phase identification task, and investigate sequential architectures, input size and dataset augmentations. The selected phases for classification were isotropic, nematic, cholesteric, and smectic. The quantities of images in all prepared sets are presented in Table 1.
\begin{table}[h]
\begin{center}
\caption{Dataset distribution for 4-phase classifier models.}
\begin{tabular}{l|c|c|c|c|c|c}
\toprule
& \textbf{Isotropic} & \textbf{Nematic} & \textbf{Cholesteric} & \textbf{Smectic} & \textbf{Totals} & \textbf{\% of total}\\
\midrule
\textbf{Training} & 1500 & 1691 & 1549 & 1689 & 6429 & 71.16\\
\textbf{Validation} & 400 & 471 & 405 & 457 & 1733 & 19.18\\
\textbf{Test} & 200 & 208 & 178 & 287 & 873 & 9.66\\
\midrule
\textbf{Totals} & 2100 & 2370 & 2132 & 2433 & 9035\\
\bottomrule
\end{tabular}
\end{center}
\end{table}
All images were prepared as in Section 6.2, aside from the isotropic phase, which is completely dark under a polarised microscope. Isotropic images were generated as dark noise by randomly selecting the value of each pixel from the uniform distribution $[0,0.1]$.
\subsection{Model architectures and training configurations}
A total of 24 models were trained on the 4-phase dataset. Two input image sizes, $256 \times 256$ and $128 \times 128$, as well as flip and all augmentations were tested for six sequential CNNs, with an increasing number of convolutional layers from one to six.  The loss function used for every model is the categorical cross-entropy. After some preliminary trial and error model training the following settings were decided on. The input is convolved with 32 kernels, resulting in 32 channels for the first convolutional layer of each model. Two kernels are used for all others layers, doubling the number of channels with each successive convolutional layer. Each convoluational layer has kernel size $3 \times 3$ and same padding, and is followed by a standard pooling layer, aside from the final convolutional layer which is followed by global average pooling. This is then followed by a dense layers with 256 units, then 128 units, and 4 units for the output dense layer which has a softmax activation function. As an example, an architectural diagram of the three-layer model is displayed in Figure 3.
\begin{figure}[h]
\centering
\includegraphics[width=5.5812in]{images/v3_conv_3_diagram.png}
\caption{Architectural diagram of the sequential CNN model with input size $256 \times 256$ and three convolutional layers. The output dimensions are displayed as width $\times$ height $\times$ channels for each convolutional and max pooling layer. Not to scale.}
\end{figure}
\subsection{Results}
The final accuracy of a trained model will vary with each training attempt due to the stochastic nature of parameter initialisation, data augmentation and batch selection. Each of the 24 models were trained three times, and the accuracies calculated for both validation and test sets. The overall validation and test accuracies of a model are taken as the mean of the three attempts, with an uncertainty of half the range in accuracy due to the small sample size. The final results are displayed in Figure 4.
\begin{figure}[h]
	\centering
    \includegraphics[width=6.6in]{images/4-phase_graphs.png}
    \caption{Plots of the mean accuracies against number of convolutional layers for the 4-phase models over three training runs.}
\end{figure} 
%general increase in accuracy for validation vs decrease for test
%suggests overfitting with higher numbers of layers
%divergence of test and validation accuracy in all models except 256 flip
%flip augmentation better than all
%similar performance between both input sizes
%relatively small error bars on validation suggests good model stability
%need more trial runs for better picture of accuracy
%high overall accuracies show viability
%best test set accuracy, flip 256 2 layers
%isotropic always predicted correctly, possible leakage giving false indication of high %accuracy
%using flip 256 for other models
From Figure 4, we see that in general the validation accuracy has a slight positive correlation with the number of layers, with the opposite for the test accuracy. This trend is not so clear for the case of flip augmentations with $256 \times 256$ input size. The decrease in test set accuracy with higher numbers of layers is suggestive of overfitting, which is to be expected as the model capacity increases greatly with each added convolutional layer. The lower capacity models from this selection are, therefore, a better choice. The high validation accuracies at greater layer counts is potentially because of the fact that the models are saved at the point of highest validation accuracy during training, which can fluctuate to high numbers.

In every case, for the same input size and number of layers, models with flip augmentations have a higher mean test set accuracy than with all augmentations. This suggests that the some of the images produced by the extra augmentations are dissimilar to actual texture images. When applying the rotation, scaling, and translation transformations included in all augmentations, certain edges and corners of the image are extended or distorted to fill the input space. These regions do not contain features identifiable as liquid crystal texture, which could be the cause of the lower performance.

With the same augmentations and number of layers, 5 in 6 models with an input size of $256 \times 256$ performed better on the test set than $128 \times 128$. Certain textural features could have potentially been lost at the lower resolution, which could explain the lower accuracies observed.

Overall, the models achieved good accuracies on the test set, with 10 out 24 scoring above 90 percent. This demonstrates the viability of CNNs in the liquid crystal phase identification task. However, the textures of the four phases these models were trained to classify are highly distinct and potentially easily identified by the human eye, meaning even the best models would perhaps struggle to compete.

The highest accuracy on the test set out of every trained model was 94.33 percent. This was one of the models with two convolutional layers, flip augmentations, and $256 \times 256$ input size. The lowest had 74.42 percent accuracy, with 6 convolutional layers, all augmentations, and $128 \times 128$ input size. Figure 5 displays the test set confusion matrices for these models.
\begin{figure}[h]
	\centering
    \includegraphics[width=5.5in]{images/confusion_matrix.png}
    \caption{Test set confusion matrices for the models with highest and lowest overall test accuracies. The value in each square represents the fraction of examples with the true phase label for which the model output was the predicted phase label. We therefore aim for values close to one along the diagonal, which corresponds to correct model outputs.}
\end{figure} 
We see that both models, especially the less accurate one, perform the worst on the cholesteric and smectic phases, suggesting that they have a higher degree of similarity between their textural features than the other classes. Both models identify the nematic phase more than 95 percent of the time, and are completely accurate in identifying the isotropic phase and do not miss-label any other phases as isotropic. This high success rate is potentially a result of the uniformity of the generated dark isotropic textures, which do not have highly complex features for the model to learn. Upon removal of the isotropic class from the test set, the most accurate model's accuracy is reduced to 92.57 percent, and the least accurate to 66.86. The isotropic class is, therefore, potentially a source of data leakage. However, it does suggest that the models would easily be trained to identify real isotropic textures. 
%=========================================================================================
\section{Smectic A and C binary classifier models}
\subsection{Dataset distribution}
The next set of models were an attempt at the binary classification task of smectic A and C phases, which can have similar textural features due to their structures only differing by a tilt in the director \cite{Dierking03}. The distribution of data for the two classes is displayed in Table 2.
\begin{table}[h]
\begin{center}
\caption{Dataset distribution for smectic A and C classifier models.}
\begin{tabular}{l|c|c|c|c}
\toprule
& \textbf{Smectic A} & \textbf{Smectic C} & \textbf{Totals} & \textbf{\% of total}\\
\midrule
\textbf{Training} & 719 & 1067 & 1786 & 70.17\\
\textbf{Validation} & 174 & 183 & 357 & 14.03\\
\textbf{Test} & 204 & 198 & 402 & 15.80\\
\midrule
\textbf{Totals} & 1097 & 1448 & 2545\\
\bottomrule
\end{tabular}
\end{center}
\end{table}
All images were again prepared as in Section 6.2.
\subsection{Model architectures and training configuration}
Based on the results of Section 7, flip augmentations and an input size of $256 \times 256$ were used for all smectic A and C models. The binary cross-entropy loss function is used with one output unit for every model. The same six sequential model configurations as for the 4-phase classification task were tested, with reduced numbers of hidden units to prevent overfitting because the dataset is smaller. In this case, the final convolutional layer for each sequential model has 32 channels, with the number of channels halving with each convolutional layer going backwards through the model. Therefore, the six convolutional layer model has only one channel in its first layer. The number of hidden units in the dense layers is also reduced, with 32 in the first and 16 in the second for all sequential models.

A different type of CNN architecture was also tested for smectic A and C, inspired by Google's 22 layer GoogLeNet Inception model, the winner of the ILSVRC in 2014 \cite{ILSVRC15}. An Inception model contains branching "Inception blocks", a specific arrangement of convolutional layers depicted in Figure 6 \cite{Szegedy2015}.  
\begin{figure}[h]
\centering
\includegraphics[width=5in]{images/inception_block.png}
\caption{Architectural diagram of an Inception block. The input is shared between 4 parallel convolution operations, with their outputs being concatenated along the channel axis in the final layer. This results in the output having a number of channels equal to the sum of the number of channels of each concatenated layer. Different kernel sizes in each parallel layer can extract different sized features from the input, propagating more information through the output of the block. The $1 \times 1$ kernel layers preceeding the $3 \times 3$ and $5 \times 5$ ones can be used to reduce the number of channels and, therefore, the computational cost, if necessary. The pooling layer is there to provide some translational invariance \cite{Szegedy2015}.}
\end{figure}
Three Inception models were used for smectic A and C, containing one, two and three Inception blocks. Similarly to GoogLeNet, Each one starts with a convolutional layer with kernel size $7 \times 7$, 2 channels and same padding, followed by a standard pooling layer, followed by two more convolutional layers, first with kernel size $1 \times 1$, 4 channels and valid padding, and second with kernel size $3 \times 3$, 8 channels and same padding, followed by a standard pooling layer. This is proceeded by the Inception blocks. The number of channels in each layer in a block is reduced to a set amount by the $1 \times 1$ kernel layers, and starts at 8 for the first block, doubling with each successive block. The final inception block is followed by average pooling with pool size and strides of $5 \times 5$ and valid padding, and a convolutional layer with kernel size $3 \times 3$, a number of channels equal to half that of the output of the final inception block, and same padding. Similarly to the sequential models, it is finished with global average pooling, followed by two dense layers, first with a number of hidden units equal to the channels in the previous convolutional layer, and second with half that number, before the final single output unit. As an example, the architecture of the inception model with one block is outlined in Figure 7.
\begin{figure}[!ht]
	\centering
    \includegraphics[width=5.393in]{images/inception_model.png}
    \caption{Architectural diagram of the Inception CNN model with one block. The output dimensions are displayed as width $\times$ height $\times$ channels for each convolutional, max pooling and Inception block layer. Not to scale.}
\end{figure} 
\subsection{Results}
Similarly to the 4-phase models, all sequential and Inception smectic A and C models were trained three times and the mean validation and test accuracies calculated, with an uncertainty of half the range. These results are shown in Figure 8. 
\begin{figure}[!htb]
	\centering
    \includegraphics[width=5.6678in]{images/smecticAC_graphs.png}
    \caption{Plots of the mean accuracies over three training runs against number of convolutional layers for sequential models, and number of Inception blocks for Inception models, for the smectic A and C models.}
%no noticeable trends in accuracies for either type

\end{figure} 
%=========================================================================================
\section{General smectic phase classifier models}
\subsection{Dataset distribution}
a
\subsection{Model architectures and training configuration}
a
\subsection{Results}
a
%=========================================================================================
\section{Conclusions}
a
%=========================================================================================
\section{Going forward}
a
%=========================================================================================

\bibliography{report}

\appendix
\appendixpage
\section{Backpropagation}
\section{Adam optimiser}
Before the first iteration, the biased first and second moment estimate variables $\bm{s}$ and $\bm{r}$ are initialised to zero. At the start of an iteration, a batch of data is sampled and $\hat{\bm{g}}$ is calculated as in Equation 13. $\bm{s}$ is updated as
\begin{equation}
\bm{s}
\end{equation}        

\end{document}